{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "920780e4-3581-446c-9673-1e2069015ad5",
   "metadata": {},
   "source": [
    "--- Data Extraction from PDF and Data Prep---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4010290-0dc5-40dd-a921-f31c8fc10855",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd29efb7-73c3-4d55-bea7-3400c3c7e758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. CONFIGURATION ---\n",
    "pdf_file_path = \"/Users/berkeelibol/anaconda_projects/3cd054a8-6e0f-4baf-88d9-78b83253cb18/thoreau_complete_journals.pdf\"\n",
    "output_csv = \"thoreau_journals_v18.csv\"\n",
    "\n",
    "MAX_WORD_COUNT = 250\n",
    "MIN_WORD_COUNT = 6   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cc90e13-467d-4f91-b322-836ed1e9b8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading single file: /Users/berkeelibol/anaconda_projects/3cd054a8-6e0f-4baf-88d9-78b83253cb18/thoreau_complete_journals.pdf\n",
      "Found 4276 entries based on dates.\n",
      "\n",
      "SUCCESS. Saved 9679 rows to: thoreau_journals_v18.csv\n",
      "['12-20', '09-13', '07-10', '07-02', '08-14', '04-03', '08-06', '12-17', '10-17', '05-16']\n"
     ]
    }
   ],
   "source": [
    "# --- 2. REGEX PATTERNS ---\n",
    "\n",
    "# A. DATE SPLITTER\n",
    "# Matches: \"March 21\", \"Mar. 21\", \"April 5\", \"Apr 5\", \"Sept. 21\"\n",
    "# added [a-z]* back to allow full month names.\n",
    "date_split_pattern = (\n",
    "    r'(?:\\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sept?|Oct|Nov|Dec)' \n",
    "    r'[a-z]*'           # ALLOWS extra letters (e.g. 'uary' in January, 'ch' in March)\n",
    "    r'\\.?'              # Optional dot\n",
    "    r'\\s*'              # Optional space\n",
    "    r'[\\'’]?\\s*'        # Optional apostrophe\n",
    "    r'\\d{1,2}'          # Digits (Day)\n",
    "    r'(?:st|nd|rd|th)?\\.?)' # Optional ordinal/dot\n",
    ")\n",
    "\n",
    "# B. EDITORIAL REMOVAL\n",
    "# Cleans headers, editorial notes in brackets etc.\n",
    "editorial_bracket_pattern = re.compile(r'\\[.*?\\]', re.DOTALL)\n",
    "citation_pattern = re.compile(r'(?:Week,\\s*p\\.[^;]*;?|Riv\\.[^.]*\\.?|Vol\\.\\s+[IVX]+)', re.IGNORECASE)\n",
    "\n",
    "# C. HEADER DATE REMOVER\n",
    "# Headers included dates which caused unintended breaks in the text and sometimes did not match with the date of the text \n",
    "header_date_pattern = re.compile(\n",
    "    r'\\[' r'\\s*' \n",
    "    r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sept?|Oct|Nov|Dec)[a-z]*' \n",
    "    r'\\.?' r'\\s*' r'\\d{1,2}', \n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "# D. NOISE CLEANING\n",
    "time_marker_pattern = re.compile(r'\\b(?:\\d{1,2}\\s*)?[AP]\\.?\\s*M\\.?\\s*[-—–]?\\s*', re.IGNORECASE)\n",
    "\n",
    "# --- 3. HELPER FUNCTIONS ---\n",
    "\n",
    "def get_quadrant_sorted_text(page):\n",
    "# Each page consisted of 4 pages which when read mixed the order of the pages\n",
    "    \"\"\"Sorts 4-up layout correctly (Top-Left -> Top-Right -> Bottom-Left -> Bottom-Right).\"\"\"\n",
    "    blocks = page.get_text(\"blocks\")\n",
    "    if not blocks: return \"\"\n",
    "    mid_x, mid_y = page.rect.width / 2, page.rect.height / 2\n",
    "    tl, tr, bl, br = [], [], [], []\n",
    "    for b in blocks:\n",
    "        x0, y0 = b[0], b[1]\n",
    "        if y0 < mid_y:\n",
    "            if x0 < mid_x: tl.append(b)\n",
    "            else: tr.append(b)\n",
    "        else:\n",
    "            if x0 < mid_x: bl.append(b)\n",
    "            else: br.append(b)\n",
    "    for q in [tl, tr, bl, br]: q.sort(key=lambda b: b[1])\n",
    "    return \"\\n\".join([b[4] for b in tl + tr + bl + br])\n",
    "\n",
    "def clean_text_block(text):\n",
    "#Cleans interfering text in headers/footers etc that identified before\n",
    "    \"\"\"Cleans text BEFORE splitting.\"\"\"\n",
    "    # 1. Remove Header Dates (e.g. \"[March 21\")\n",
    "    text = header_date_pattern.sub(' ', text)\n",
    "    \n",
    "    # 2. Remove Editorial Brackets (e.g. \"[Week, p. 34]\")\n",
    "    text = editorial_bracket_pattern.sub(' ', text)\n",
    "    \n",
    "    # 3. Remove Citation Noise (e.g. \"Riv. 12.\")\n",
    "    text = citation_pattern.sub(' ', text)\n",
    "    \n",
    "    # 4. Remove All-Caps Titles\n",
    "    lines = text.split('\\n')\n",
    "    clean_lines = []\n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "        if not stripped: continue\n",
    "        # Filter: Long, All-Caps, No Lowercase\n",
    "        if len(stripped) > 5 and not re.search(r'[a-z]', stripped): \n",
    "             continue \n",
    "        if re.match(r'^\\d+$', stripped): continue # Page numbers\n",
    "        clean_lines.append(line)\n",
    "        \n",
    "    return \"\\n\".join(clean_lines)\n",
    "\n",
    "def post_process_content(text):\n",
    "    \"\"\"Cleans the specific entry content.\"\"\"\n",
    "    text = time_marker_pattern.sub('', text) \n",
    "    text = re.sub(r'\\d+', ' ', text)         \n",
    "    text = re.sub(r'[\\[\\]]', ' ', text)      \n",
    "    text = re.sub(r'¬\\s*', '', text)         \n",
    "    text = re.sub(r'(\\w)-\\s+(\\w)', r'\\1\\2', text) \n",
    "    text = re.sub(r'\\s+', ' ', text).strip() \n",
    "    return text\n",
    "\n",
    "def parse_date_mm_dd(date_str):\n",
    "#The dates throughout the tex included dates as text, converts text dates into numerical format\n",
    "    \"\"\"Converts 'March 21' -> '03-21'\"\"\"\n",
    "    if not date_str: return None\n",
    "    # Normalize\n",
    "    clean_str = date_str.replace('.', ' ').replace(\"'\", ' ').replace('’', ' ')\n",
    "    clean_str = re.sub(r'([a-zA-Z]+)(\\d+)', r'\\1 \\2', clean_str)\n",
    "    parts = clean_str.split()\n",
    "    if len(parts) < 2: return None\n",
    "    \n",
    "    # Handle full month names by taking just first 3 chars\n",
    "    # \"March\" -> \"Mar\", \"April\" -> \"Apr\"\n",
    "    month_str = parts[0][:3].title()\n",
    "    day_str = re.sub(r'\\D', '', parts[1]) \n",
    "    \n",
    "    try:\n",
    "        m = datetime.strptime(month_str, \"%b\").month\n",
    "        d = int(day_str)\n",
    "        return f\"{m:02d}-{d:02d}\"\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def sentence_aware_chunking(text, limit):\n",
    "#Chunks the text respecting to sentence completeness\n",
    "    words = text.split()\n",
    "    if len(words) <= limit: return [text]\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    chunks, curr, clen = [], [], 0\n",
    "    for s in sentences:\n",
    "        slen = len(s.split())\n",
    "        if clen + slen > limit:\n",
    "            if curr: chunks.append(\" \".join(curr))\n",
    "            curr, clen = [s], slen\n",
    "        else:\n",
    "            curr.append(s)\n",
    "            clen += slen\n",
    "    if curr: chunks.append(\" \".join(curr))\n",
    "    return chunks\n",
    "\n",
    "# --- 4. MAIN EXECUTION ---\n",
    "\n",
    "print(f\"Reading single file: {pdf_file_path}\")\n",
    "master_data = []\n",
    "\n",
    "if os.path.exists(pdf_file_path):\n",
    "    try:\n",
    "        doc = fitz.open(pdf_file_path)\n",
    "        full_text = \"\"\n",
    "        \n",
    "        # A. EXTRACT\n",
    "        for page in doc:\n",
    "            full_text += get_quadrant_sorted_text(page) + \"\\n\"\n",
    "        \n",
    "        # B. CLEAN\n",
    "        cleaned_text = clean_text_block(full_text)\n",
    "        \n",
    "        # C. SPLIT \n",
    "        chunks = re.split(f'({date_split_pattern})', cleaned_text, flags=re.IGNORECASE)\n",
    "        print(f\"Found {len(chunks)//2} entries based on dates.\")\n",
    "        \n",
    "        last_mm_dd = None \n",
    "        \n",
    "        for i in range(1, len(chunks), 2):\n",
    "            separator = chunks[i].strip()\n",
    "            content = chunks[i+1] if i+1 < len(chunks) else \"\"\n",
    "            \n",
    "            # PARSE DATE\n",
    "            mm_dd = parse_date_mm_dd(separator)\n",
    "            if mm_dd:\n",
    "                last_mm_dd = mm_dd\n",
    "            else:\n",
    "                mm_dd = last_mm_dd\n",
    "\n",
    "            # CLEAN CONTENT\n",
    "            final_content = post_process_content(content)\n",
    "            \n",
    "            # FILTER\n",
    "            if len(final_content.split()) < MIN_WORD_COUNT: \n",
    "                continue\n",
    "            \n",
    "            # CHUNK\n",
    "            final_chunks = sentence_aware_chunking(final_content, MAX_WORD_COUNT)\n",
    "            \n",
    "            for idx, chunk in enumerate(final_chunks):\n",
    "                master_data.append({\n",
    "                    'Date_MM_DD': mm_dd,\n",
    "                    'Content': chunk,\n",
    "                    'Vertical_Selftranscendence': None,\n",
    "                    'Horizontal_Selftranscendence': None,\n",
    "                    'Self_Actualization': None,\n",
    "                    'Order': None,\n",
    "                    'Well_Being': None,\n",
    "                    'Valence_Pos': None,\n",
    "                    'Valence_Neg': None\n",
    "                })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    # --- 5. SAVE ---\n",
    "    if master_data:\n",
    "        df = pd.DataFrame(master_data)\n",
    "        df.to_csv(output_csv, index=False)\n",
    "        print(f\"\\nSUCCESS. Saved {len(df)} rows to: {output_csv}\")\n",
    "        print(df['Date_MM_DD'].sample(10).tolist())\n",
    "    else:\n",
    "        print(\"No data extracted.\")\n",
    "\n",
    "else:\n",
    "    print(\"File not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe7550e-351f-40ce-be00-6df183cd1605",
   "metadata": {},
   "source": [
    "At this stage I did some manual cleaning and prepping before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d6cb749-4ace-4298-9462-914e76a06ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 9639 rows.\n",
      "------------------------------\n",
      "SUCCESS!\n",
      "1. 'thoreau_gold_standard_200.csv' created with 200 rows.\n",
      "2. 'thoreau_training_seed_50.csv' created with 50 rows.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURATION ---\n",
    "input_csv = \"/Users/berkeelibol/anaconda_projects/3cd054a8-6e0f-4baf-88d9-78b83253cb18/thoreau_complete_clean.csv\"\n",
    "gold_standard_file = \"thoreau_gold_standard_200.csv\"\n",
    "training_seed_file = \"thoreau_training_seed_50.csv\"\n",
    "\n",
    "# --- MAIN EXECUTION ---\n",
    "df = pd.read_csv(input_csv)\n",
    "print(f\"Loaded dataset with {len(df)} rows.\")\n",
    "\n",
    "# 1. THE GOLD STANDARD (Test Set)\n",
    "test_set = df.sample(n=200, random_state=42)\n",
    "\n",
    "# 2. THE TRAINING SEED -> Still 50 (Small start for Active Learning)\n",
    "# We drop the test_set indices first so no data leaks from Test to Train\n",
    "remaining_pool = df.drop(test_set.index)\n",
    "train_seed = remaining_pool.sample(n=50, random_state=42)\n",
    "\n",
    "# 3. SAVE\n",
    "test_set.to_csv(gold_standard_file, index=False)\n",
    "train_seed.to_csv(training_seed_file, index=False)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"SUCCESS!\")\n",
    "print(f\"1. '{gold_standard_file}' created with {len(test_set)} rows.\")\n",
    "print(f\"2. '{training_seed_file}' created with {len(train_seed)} rows.\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a6291e-cd93-4947-a8d4-72c8dd3b501b",
   "metadata": {},
   "source": [
    "I created one initial training set and one gold standart set to test accuracy later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80e1d28-358f-4c63-a6b0-2f23f0b68778",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install setfit pandas numpy torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39765b75-aa60-4c2b-8d6d-d9a47512c0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from setfit import SetFitModel, SetFitTrainer\n",
    "from datasets import Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14b40bf2-1e85-4350-86dc-8a3a01d3e2ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. LOADING DATA ---\n",
      "Labeled Seeds: 50\n",
      "Unlabeled Pool: 9589\n",
      "\n",
      "Training on: ['Vertical_Selftranscendence', 'Self_Actualization', 'Order', 'Well_Being']\n",
      "(!) SKIPPING: ['Horizontal_Selftranscendence'] (0 positive examples found)\n",
      "\n",
      "--- 2. TRAINING MODEL ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "/var/folders/h6/7f173jxs0594zn_vj8c95k7m0000gn/T/ipykernel_54318/748407106.py:68: DeprecationWarning: `SetFitTrainer` has been deprecated and will be removed in v2.0.0 of SetFit. Please use `Trainer` instead.\n",
      "  trainer = SetFitTrainer(\n",
      "Applying column mapping to the training dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c769c5fc073346a38a000dbbf8ac7749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num unique pairs = 2000\n",
      "  Batch size = 4\n",
      "  Num epochs = 1\n",
      "/opt/anaconda3/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 09:09, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.180200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.177200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.138500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.049700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.044100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.040200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.050700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.030100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.045900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.061100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.044000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "\n",
      "--- 3. RUNNING INFERENCE ---\n",
      "\n",
      "--- 4. SELECTING NEXT BATCH ---\n",
      "------------------------------\n",
      "Saved 'round_2_to_label.csv' with 50 rows.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURATION ---\n",
    "SEED_FILE = \"seed_training_labeled.csv\"\n",
    "FULL_DATA_FILE = \"thoreau_complete_clean.csv\"\n",
    "OUTPUT_NEXT_ROUND = \"round_2_to_label.csv\"\n",
    "MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\" \n",
    "\n",
    "# The columns to train on\n",
    "TARGET_COLS = [\n",
    "    'Vertical_Selftranscendence', \n",
    "    'Horizontal_Selftranscendence', \n",
    "    'Self_Actualization', \n",
    "    'Order', \n",
    "    'Well_Being'\n",
    "]\n",
    "\n",
    "# Keywords to find missing Horizontal examples, it was the only category that had no examples in the training set so i added this to the logic.\n",
    "HORIZONTAL_KEYWORDS = [\"infinite\", \"universe\", \"eternal\", \"all\", \"connection\", \"whole\", \"boundless\", \"cosmos\"]\n",
    "\n",
    "def train_active_learning():\n",
    "    print(\"--- 1. LOADING DATA ---\")\n",
    "    df_seed = pd.read_csv(SEED_FILE)\n",
    "    df_full = pd.read_csv(FULL_DATA_FILE)\n",
    "    \n",
    "    # Filter out labeled rows from the full pool\n",
    "    labeled_content = set(df_seed['Content'].tolist())\n",
    "    df_unlabeled = df_full[~df_full['Content'].isin(labeled_content)].copy()\n",
    "    \n",
    "    print(f\"Labeled Seeds: {len(df_seed)}\")\n",
    "    print(f\"Unlabeled Pool: {len(df_unlabeled)}\")\n",
    "\n",
    "    # --- 2. CHECKING CLASS BALANCE ---\n",
    "    trainable_cols = []\n",
    "    skipped_cols = []\n",
    "    \n",
    "    for col in TARGET_COLS:\n",
    "        # Check if we have at least one '1' for this category\n",
    "        if df_seed[col].sum() > 0:\n",
    "            trainable_cols.append(col)\n",
    "        else:\n",
    "            skipped_cols.append(col)\n",
    "            \n",
    "    print(f\"\\nTraining on: {trainable_cols}\")\n",
    "    if skipped_cols:\n",
    "        print(f\"(!) SKIPPING: {skipped_cols} (0 positive examples found)\")\n",
    "\n",
    "    # --- 3. TRAINING SETFIT ---\n",
    "    \n",
    "    # Create 'label' column as list of [0, 1, 0...] for the trainable columns\n",
    "    df_seed['label'] = df_seed[trainable_cols].values.tolist()\n",
    "    \n",
    "    # Convert to Hugging Face Dataset \n",
    "    train_dataset = Dataset.from_pandas(df_seed)\n",
    "\n",
    "    print(\"\\n--- 2. TRAINING MODEL ---\")\n",
    "    # Load SetFit Model (Multi-label enabled by one-vs-rest strategy)\n",
    "    model = SetFitModel.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        multi_target_strategy=\"one-vs-rest\"\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    trainer = SetFitTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        column_mapping={\"Content\": \"text\", \"label\": \"label\"},\n",
    "        num_iterations=20, \n",
    "        batch_size=4,\n",
    "        num_epochs=1\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "    # --- 4. INFERENCE ON UNLABELED POOL ---\n",
    "    print(\"\\n--- 3. RUNNING INFERENCE ---\")\n",
    "    \n",
    "    # Predict probabilities, returns a matrix of probs\n",
    "    probs = model.predict_proba(df_unlabeled['Content'].tolist())\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df_probs = pd.DataFrame(probs, columns=trainable_cols, index=df_unlabeled.index)\n",
    "    \n",
    "    # --- 5. ACTIVE LEARNING STRATEGY ---\n",
    "    print(\"\\n--- 4. SELECTING NEXT BATCH ---\")\n",
    "    \n",
    "    # A. UNCERTAINTY SAMPLING\n",
    "    uncertainty_scores = 1 - (df_probs - 0.5).abs() * 2\n",
    "    df_unlabeled['max_uncertainty'] = uncertainty_scores.max(axis=1)\n",
    "    \n",
    "    # Select Top 30 Most Uncertain\n",
    "    uncertain_candidates = df_unlabeled.nlargest(30, 'max_uncertainty')\n",
    "    uncertain_candidates = uncertain_candidates.copy()\n",
    "    uncertain_candidates['Reason'] = 'Uncertainty'\n",
    "    \n",
    "    # B. KEYWORD DISCOVERY (RESTORED THIS BLOCK)\n",
    "    # Looks for rows containing keywords but NOT already in the uncertain set\n",
    "    keyword_mask = df_unlabeled['Content'].str.contains('|'.join(HORIZONTAL_KEYWORDS), case=False, na=False)\n",
    "    keyword_candidates = df_unlabeled[keyword_mask].drop(uncertain_candidates.index, errors='ignore').head(20)\n",
    "    keyword_candidates = keyword_candidates.copy()\n",
    "    keyword_candidates['Reason'] = 'Keyword_Rescue'\n",
    "    \n",
    "    # Combine\n",
    "    next_batch = pd.concat([uncertain_candidates, keyword_candidates])\n",
    "    \n",
    "    # Clean up for export\n",
    "    for col in TARGET_COLS:\n",
    "        next_batch[col] = \"\" # Empty columns for manual labeling\n",
    "        \n",
    "    output_cols = ['Year', 'Date_MM_DD', 'Content', 'Reason'] + TARGET_COLS\n",
    "    next_batch = next_batch[output_cols]\n",
    "    \n",
    "    # --- 6. SAVE ---\n",
    "    next_batch.to_csv(OUTPUT_NEXT_ROUND, index=False)\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Saved '{OUTPUT_NEXT_ROUND}' with {len(next_batch)} rows.\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_active_learning()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f11a447-5218-4e50-ac0e-00926d56737f",
   "metadata": {},
   "source": [
    "At this point I label the round 2 data manually and import it as a new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2761c23b-a3cd-4177-9f01-4b5fcb763361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. MERGING 2 LABELED FILES ---\n",
      "  -> Loaded seed_training_labeled.csv (50 rows)\n",
      "  -> Loaded round_2_labeled.csv (50 rows)\n",
      "Total Training Examples: 100\n",
      "Unlabeled Pool Remaining: 9539\n",
      "Training on: ['Vertical_Selftranscendence', 'Horizontal_Selftranscendence', 'Self_Actualization', 'Order', 'Well_Being']\n",
      "\n",
      "--- 2. TRAINING MODEL (ITERATION) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "/var/folders/h6/7f173jxs0594zn_vj8c95k7m0000gn/T/ipykernel_54318/2154205143.py:87: DeprecationWarning: `SetFitTrainer` has been deprecated and will be removed in v2.0.0 of SetFit. Please use `Trainer` instead.\n",
      "  trainer = SetFitTrainer(\n",
      "Applying column mapping to the training dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c33b21637a1405fa6284a73a3bd6358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num unique pairs = 4000\n",
      "  Batch size = 4\n",
      "  Num epochs = 1\n",
      "/opt/anaconda3/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 2:04:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.360200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.256900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.205300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.163900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.117500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.129000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.069600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.094600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.106600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.069500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.045900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.056100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.072100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.051300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.034300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.035200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.047200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.047000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.038800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.065500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "\n",
      "--- 3. RUNNING INFERENCE ---\n",
      "\n",
      "--- 4. SELECTING NEXT BATCH ---\n",
      "------------------------------\n",
      "Saved 'round_3_to_label.csv' with 50 rows.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from setfit import SetFitModel, SetFitTrainer\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION (UPDATE THIS EVERY ROUND) ---\n",
    "LABELED_FILES = [\n",
    "    \"seed_training_labeled.csv\",\n",
    "    \"round_2_labeled.csv\"\n",
    "]\n",
    "\n",
    "FULL_DATA_FILE = \"thoreau_complete_clean.csv\"\n",
    "OUTPUT_NEXT_ROUND = \"round_3_to_label.csv\" #for Round 3, i change the number each iteraiton\n",
    "MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\" \n",
    "\n",
    "# Columns\n",
    "TARGET_COLS = [\n",
    "    'Vertical_Selftranscendence', \n",
    "    'Horizontal_Selftranscendence', \n",
    "    'Self_Actualization', \n",
    "    'Order', \n",
    "    'Well_Being'\n",
    "]\n",
    "\n",
    "def train_next_iteration():\n",
    "    print(f\"--- 1. MERGING {len(LABELED_FILES)} LABELED FILES ---\")\n",
    "    \n",
    "    # Load and combine files\n",
    "    df_list = []\n",
    "    for f in LABELED_FILES:\n",
    "        if os.path.exists(f):\n",
    "            d = pd.read_csv(f)\n",
    "            # Force content to be string to avoid any other type errors\n",
    "            d['Content'] = d['Content'].astype(str)\n",
    "            df_list.append(d)\n",
    "            print(f\"  -> Loaded {f} ({len(d)} rows)\")\n",
    "        else:\n",
    "            print(f\"  (!) ERROR: File not found: {f}\")\n",
    "            return\n",
    "\n",
    "    df_train = pd.concat(df_list, ignore_index=True)\n",
    "    df_full = pd.read_csv(FULL_DATA_FILE)\n",
    "\n",
    "    #Fill NaN with 0\n",
    "    for col in TARGET_COLS:\n",
    "        if col in df_train.columns:\n",
    "            df_train[col] = pd.to_numeric(df_train[col], errors='coerce').fillna(0).astype(int)\n",
    "        else:\n",
    "            df_train[col] = 0 \n",
    "    \n",
    "    # Filter out labeled rows\n",
    "    labeled_content = set(df_train['Content'].tolist())\n",
    "    df_unlabeled = df_full[~df_full['Content'].isin(labeled_content)].copy()\n",
    "    \n",
    "    print(f\"Total Training Examples: {len(df_train)}\")\n",
    "    print(f\"Unlabeled Pool Remaining: {len(df_unlabeled)}\")\n",
    "\n",
    "    # --- 2. CHECKING CLASS BALANCE ---\n",
    "    trainable_cols = []\n",
    "    skipped_cols = []\n",
    "    \n",
    "    for col in TARGET_COLS:\n",
    "        # Ensure numeric type for summation\n",
    "        count = pd.to_numeric(df_train[col], errors='coerce').fillna(0).sum()\n",
    "        if count > 0:\n",
    "            trainable_cols.append(col)\n",
    "        else:\n",
    "            skipped_cols.append(col)\n",
    "            \n",
    "    print(f\"Training on: {trainable_cols}\")\n",
    "    if skipped_cols:\n",
    "        print(f\"(!) SKIPPING: {skipped_cols} (Still 0 positive examples)\")\n",
    "\n",
    "    # --- 3. TRAINING ---\n",
    "    # Prepare labels\n",
    "    df_train['label'] = df_train[trainable_cols].values.tolist()\n",
    "    \n",
    "    # Kept getting errors about type for the year coloumn, decided to exclude them from the training as they arent necessary anyway\n",
    "    training_data_clean = df_train[['Content', 'label']].copy()\n",
    "    train_dataset = Dataset.from_pandas(training_data_clean)\n",
    "\n",
    "    print(\"\\n--- 2. TRAINING MODEL (ITERATION) ---\")\n",
    "    model = SetFitModel.from_pretrained(MODEL_NAME, multi_target_strategy=\"one-vs-rest\")\n",
    "    \n",
    "    trainer = SetFitTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        column_mapping={\"Content\": \"text\", \"label\": \"label\"},\n",
    "        num_iterations=20, \n",
    "        batch_size=4,\n",
    "        num_epochs=1\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "    # --- 4. INFERENCE ---\n",
    "    print(\"\\n--- 3. RUNNING INFERENCE ---\")\n",
    "    probs = model.predict_proba(df_unlabeled['Content'].tolist())\n",
    "    df_probs = pd.DataFrame(probs, columns=trainable_cols, index=df_unlabeled.index)\n",
    "    \n",
    "    # --- 5. SELECTION STRATEGY ---\n",
    "    print(\"\\n--- 4. SELECTING NEXT BATCH ---\")\n",
    "    \n",
    "    # Uncertainty Formula\n",
    "    uncertainty_scores = 1 - (df_probs - 0.5).abs() * 2\n",
    "    df_unlabeled['max_uncertainty'] = uncertainty_scores.max(axis=1)\n",
    "    \n",
    "    # Select Top 50 Most Uncertain\n",
    "    next_batch = df_unlabeled.nlargest(50, 'max_uncertainty').copy()\n",
    "    next_batch['Reason'] = 'Uncertainty'\n",
    "    \n",
    "    # Format output\n",
    "    for col in TARGET_COLS:\n",
    "        next_batch[col] = \"\"\n",
    "        \n",
    "    output_cols = ['Year', 'Date_MM_DD', 'Content', 'Reason'] + TARGET_COLS\n",
    "    \n",
    "    # Ensure all columns exist before saving, as year, date etc. was excluded from training\n",
    "    for c in output_cols:\n",
    "        if c not in next_batch.columns:\n",
    "            next_batch[c] = \"\"\n",
    "            \n",
    "    next_batch = next_batch[output_cols]\n",
    "    \n",
    "    next_batch.to_csv(OUTPUT_NEXT_ROUND, index=False)\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Saved '{OUTPUT_NEXT_ROUND}' with {len(next_batch)} rows.\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_next_iteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed1763e-f6a1-4d85-bf29-223baf0613cb",
   "metadata": {},
   "source": [
    "I repeat this process until satisfied with the outcome and rerun it one last time where I save the full dataframe."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
