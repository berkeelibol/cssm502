{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b61c1fc-29f7-4004-93d8-90c0e98f3492",
   "metadata": {},
   "source": [
    "The website I selected is Project Gutenberg, specifically the page that includes the Journal 1 of Henry David Thoreau, since I will use that data in my final project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69c1808-38db-4b81-88db-47bd8d27e2f2",
   "metadata": {},
   "source": [
    "First i install all the libraries i need, i included Regex too since I needed to extract dates from the text data i scrape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45c7a200-c37e-4205-aa27-3a8135a2fd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from scrapy.selector import Selector\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5327da2-cf3c-4590-9f72-98b43cb12f58",
   "metadata": {},
   "source": [
    "I fetch the webpage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdcc52f2-4b04-4a05-b66a-e249d92af254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webpage successfully fetched.\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.gutenberg.org/cache/epub/57393/pg57393-images.html\"\n",
    "response = requests.get(url)\n",
    "\n",
    "print(\"Webpage successfully fetched.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c3b133-e905-40e5-9a2b-57a2f1495736",
   "metadata": {},
   "source": [
    "I use the selector to bring the text from xpaths that contains the text of the journals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d46c16bf-666f-4133-a025-ceec7c252558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2057 elements to process.\n"
     ]
    }
   ],
   "source": [
    "sel = Selector(text=response.text)\n",
    "\n",
    "all_nodes = sel.xpath('//body/*')\n",
    "\n",
    "print(f\"Found {len(all_nodes)} elements to process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7f7862-dba9-4e21-b766-a649b91ea0bd",
   "metadata": {},
   "source": [
    "This is the main loop, i first defined a date regex to identify matches in the text which looks at abbrivetions of the months and if they are being followed by a number. I run the main loop on the elements of all_nodes, check if first 20 letters involve any dates, if yes start a new entry. I remove the date from that entry since my goal with the data is to train a UML and i dont want the dates to be included in the training. I also added a small loop to keep track of titles, since some journal entries included those, others were names untitled. When we end the loop, i append the last entry picked up to journal entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73e8eea3-d756-4dd8-babd-d1292fed8986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction complete. Found 396 entries.\n"
     ]
    }
   ],
   "source": [
    "journal_entries = []\n",
    "\n",
    "current_entry = None\n",
    "last_seen_title = \"Untitled\"\n",
    "\n",
    "date_regex = re.compile(r'(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sept|Oct|Nov|Dec)[a-z.]*\\s\\d{1,2}', re.IGNORECASE)\n",
    "\n",
    "for node in all_nodes:\n",
    "    html_class = node.xpath('@class').get()\n",
    "    tag_name = node.root.tag\n",
    "    text_content = node.xpath('string(.)').get().strip()\n",
    "\n",
    "    if html_class == 'p2 center':\n",
    "        last_seen_title = text_content\n",
    "        continue \n",
    "\n",
    "    if tag_name == 'p':\n",
    "        \n",
    "        date_match = date_regex.match(text_content[:20])\n",
    "        \n",
    "        if date_match:\n",
    "            if current_entry:\n",
    "                journal_entries.append(current_entry)\n",
    "            \n",
    "            found_date = date_match.group(0)\n",
    "            clean_text = text_content.replace(found_date, '', 1).strip()\n",
    "            \n",
    "\n",
    "            clean_text = re.sub(r'^[.,-]\\s*', '', clean_text)\n",
    "\n",
    "            current_entry = {\n",
    "                'Title': last_seen_title, \n",
    "                'Date': found_date,       \n",
    "                'Content': clean_text     \n",
    "            }\n",
    "            \n",
    "            last_seen_title = \"Untitled\"\n",
    "            \n",
    "        else:\n",
    "            if current_entry:\n",
    "                current_entry['Content'] += \" \" + text_content\n",
    "\n",
    "if current_entry:\n",
    "    journal_entries.append(current_entry)\n",
    "\n",
    "print(f\"Extraction complete. Found {len(journal_entries)} entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46078f69-66ba-43c4-b29d-354183c53a39",
   "metadata": {},
   "source": [
    "Lastly I put all the entries in a pandas dataframe and save as csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c89ad69-c8d7-4690-93bd-0416aad944c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Title     Date  \\\n",
      "0                   Untitled   Oct 22   \n",
      "1  THE MOULD OUR DEEDS LEAVE  Oct. 24   \n",
      "2                     SPRING   Oct 25   \n",
      "3                   THE POET   Oct 26   \n",
      "4                    THE FOG  Oct. 27   \n",
      "\n",
      "                                             Content  \n",
      "0  \"What are you doing now?\" he asked.\\r\\n\"Do you...  \n",
      "1  Every part of nature teaches that the passing\\...  \n",
      "2  She appears, and we are once more children;\\r\\...  \n",
      "3   \"A noble man has not to thank a private circl...  \n",
      "4  The prospect is limited to Nobscot and\\r\\nAnnu...  \n",
      "File saved as 'thoreau_data.csv'\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(journal_entries)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "df.to_csv('thoreau_data.csv', index=False)\n",
    "\n",
    "print(\"File saved as 'thoreau_data.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba604bd3-325e-4905-abd6-e07028db6f0f",
   "metadata": {},
   "source": [
    "When I check the data it looks quite clean. The data is well organized by dates and no date entry seems to be skipped. However, i suspect the writer himself might skipped some dates especially towards the end, some entries are entirely too long, and I dont have many data points. I need to find the most logical way to seperate those long rows. \n",
    "\n",
    "My top picks right now are to seperate the data into rows not by date directly but by paragraph ends. This way i will have more managable units and a much larger quantitiy of unit-text for training. I can impute the dates for undated entries then either by repeating the previous date entry (e.g. multiple rows are March 13), or estimating based on the date of entry before and after (e.g. if 7 undated entries exist between march 13 and june 16, those are scattered in between with equal distances). I also consider expanding my data set with other volumes of his journal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
